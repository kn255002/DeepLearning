{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Credit_Card_Fraud_Detection_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DcIIUidkgQMR",
        "-aVY0NfJgQMY"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcIIUidkgQMR"
      },
      "source": [
        "# Credit Card Fraud Detection::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoUbY5SZgQMX"
      },
      "source": [
        "Download dataset from this link:\n",
        "\n",
        "https://www.kaggle.com/mlg-ulb/creditcardfraud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aVY0NfJgQMY"
      },
      "source": [
        "# Description about dataset::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5JKjSpngQMY"
      },
      "source": [
        "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
        "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. \n",
        "\n",
        "\n",
        "### Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsfQk8KngQMZ"
      },
      "source": [
        "# WORKFLOW :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g97HlYVEgQMa"
      },
      "source": [
        "1.Load Data\n",
        "\n",
        "2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n",
        "\n",
        "3.Standardized the Input Variables. \n",
        "\n",
        "4.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n",
        "\n",
        "5.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).\n",
        "\n",
        "6.Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)\n",
        "\n",
        "7.Train the Model with Epochs (100).\n",
        "\n",
        "8.If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .\n",
        "\n",
        "9.Prediction should be > 92%\n",
        "10.Evaluation Step\n",
        "11Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdDEsgaSjD6V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMA7kqgkgQMa"
      },
      "source": [
        "# Task::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBuW37DIgQMa"
      },
      "source": [
        "## Identify fraudulent credit card transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhD3UK-hmAN"
      },
      "source": [
        "**1. Load Data**\n",
        "Loading and immediately shuffeling Data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CsQJ-Z4RgQMa",
        "outputId": "7ca7880c-79e5-4b36-bdef-763ffff183a4"
      },
      "source": [
        "# Prediction Model developed by:\n",
        "# Khurram Nazir\n",
        "#  \n",
        "#\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "#\n",
        "\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_file = files.upload()\n",
        "#df = pd.read_csv(\"C:/Users/khurr/Documents/GitHub/AI-Engineering/PIAIC/Quarter-2/DeepLearning/CreditCardFraudDetection/creditcard.csv\",sep=',')\n",
        "df = pd.DataFrame(pd.read_csv(io.BytesIO(uploaded_file['creditcard.csv']),sep=','))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fa21f545-1c6a-4de2-b49f-f56da5b5391b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fa21f545-1c6a-4de2-b49f-f56da5b5391b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving creditcard.csv to creditcard.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qreMKOKSm491"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "UmL4NfbRgQMb",
        "outputId": "4fe922e5-8e80-4e40-d29e-6cdd050dcee0"
      },
      "source": [
        "df = df.sample(frac=1) #Shuffeling DF. \n",
        "df1=df #Aviding to reload data again & again from file.\n",
        "df"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64185</th>\n",
              "      <td>51067.0</td>\n",
              "      <td>0.536717</td>\n",
              "      <td>0.090463</td>\n",
              "      <td>1.134899</td>\n",
              "      <td>1.724024</td>\n",
              "      <td>-0.635885</td>\n",
              "      <td>0.793883</td>\n",
              "      <td>-0.421369</td>\n",
              "      <td>0.428730</td>\n",
              "      <td>1.024854</td>\n",
              "      <td>0.020049</td>\n",
              "      <td>0.664470</td>\n",
              "      <td>1.551671</td>\n",
              "      <td>-0.445985</td>\n",
              "      <td>-0.680214</td>\n",
              "      <td>-2.402238</td>\n",
              "      <td>-1.206342</td>\n",
              "      <td>0.709892</td>\n",
              "      <td>-0.741002</td>\n",
              "      <td>0.849804</td>\n",
              "      <td>-0.236664</td>\n",
              "      <td>-0.225364</td>\n",
              "      <td>-0.208870</td>\n",
              "      <td>0.412062</td>\n",
              "      <td>0.199573</td>\n",
              "      <td>-1.018516</td>\n",
              "      <td>-0.675466</td>\n",
              "      <td>-0.068988</td>\n",
              "      <td>-0.122334</td>\n",
              "      <td>20.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36436</th>\n",
              "      <td>38535.0</td>\n",
              "      <td>-1.185253</td>\n",
              "      <td>0.397230</td>\n",
              "      <td>0.636781</td>\n",
              "      <td>-2.139428</td>\n",
              "      <td>-0.454006</td>\n",
              "      <td>-0.224041</td>\n",
              "      <td>0.502780</td>\n",
              "      <td>0.603537</td>\n",
              "      <td>0.854961</td>\n",
              "      <td>-1.773599</td>\n",
              "      <td>0.458254</td>\n",
              "      <td>0.099457</td>\n",
              "      <td>-2.523987</td>\n",
              "      <td>0.965988</td>\n",
              "      <td>-0.131108</td>\n",
              "      <td>-0.365195</td>\n",
              "      <td>-0.119036</td>\n",
              "      <td>0.229618</td>\n",
              "      <td>0.097433</td>\n",
              "      <td>-0.489503</td>\n",
              "      <td>-0.015689</td>\n",
              "      <td>-0.144561</td>\n",
              "      <td>0.060228</td>\n",
              "      <td>-0.377304</td>\n",
              "      <td>0.016670</td>\n",
              "      <td>-1.005483</td>\n",
              "      <td>-0.066212</td>\n",
              "      <td>0.009247</td>\n",
              "      <td>88.97</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274201</th>\n",
              "      <td>165899.0</td>\n",
              "      <td>1.866748</td>\n",
              "      <td>0.011582</td>\n",
              "      <td>-1.781247</td>\n",
              "      <td>1.215596</td>\n",
              "      <td>0.730180</td>\n",
              "      <td>-0.378462</td>\n",
              "      <td>0.619591</td>\n",
              "      <td>-0.234691</td>\n",
              "      <td>-0.185538</td>\n",
              "      <td>0.368097</td>\n",
              "      <td>0.851455</td>\n",
              "      <td>1.352516</td>\n",
              "      <td>0.376523</td>\n",
              "      <td>0.593604</td>\n",
              "      <td>-1.244699</td>\n",
              "      <td>-0.546856</td>\n",
              "      <td>-0.430146</td>\n",
              "      <td>-0.209207</td>\n",
              "      <td>0.042203</td>\n",
              "      <td>-0.081348</td>\n",
              "      <td>0.145675</td>\n",
              "      <td>0.456557</td>\n",
              "      <td>-0.041958</td>\n",
              "      <td>0.731332</td>\n",
              "      <td>0.442545</td>\n",
              "      <td>-0.552150</td>\n",
              "      <td>-0.031393</td>\n",
              "      <td>-0.051858</td>\n",
              "      <td>74.86</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94663</th>\n",
              "      <td>64978.0</td>\n",
              "      <td>-0.397299</td>\n",
              "      <td>0.811866</td>\n",
              "      <td>1.284498</td>\n",
              "      <td>0.879811</td>\n",
              "      <td>0.076744</td>\n",
              "      <td>-0.665860</td>\n",
              "      <td>1.250132</td>\n",
              "      <td>-0.362702</td>\n",
              "      <td>-0.419350</td>\n",
              "      <td>0.123263</td>\n",
              "      <td>-0.428271</td>\n",
              "      <td>-0.231698</td>\n",
              "      <td>0.004358</td>\n",
              "      <td>0.113895</td>\n",
              "      <td>1.057510</td>\n",
              "      <td>-0.400089</td>\n",
              "      <td>-0.300428</td>\n",
              "      <td>0.111855</td>\n",
              "      <td>0.385011</td>\n",
              "      <td>0.403862</td>\n",
              "      <td>0.111049</td>\n",
              "      <td>0.505111</td>\n",
              "      <td>0.005072</td>\n",
              "      <td>0.384240</td>\n",
              "      <td>-0.183635</td>\n",
              "      <td>-0.348163</td>\n",
              "      <td>0.222827</td>\n",
              "      <td>0.009893</td>\n",
              "      <td>89.55</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224225</th>\n",
              "      <td>143701.0</td>\n",
              "      <td>-1.087965</td>\n",
              "      <td>0.309144</td>\n",
              "      <td>-1.424678</td>\n",
              "      <td>-0.002609</td>\n",
              "      <td>0.867860</td>\n",
              "      <td>-0.949465</td>\n",
              "      <td>-0.252551</td>\n",
              "      <td>-0.497153</td>\n",
              "      <td>0.209420</td>\n",
              "      <td>0.138327</td>\n",
              "      <td>0.527635</td>\n",
              "      <td>0.389882</td>\n",
              "      <td>-1.348132</td>\n",
              "      <td>1.208748</td>\n",
              "      <td>-0.077758</td>\n",
              "      <td>-0.436654</td>\n",
              "      <td>-0.127048</td>\n",
              "      <td>0.575521</td>\n",
              "      <td>0.878454</td>\n",
              "      <td>-0.633939</td>\n",
              "      <td>0.932414</td>\n",
              "      <td>0.323533</td>\n",
              "      <td>-0.441924</td>\n",
              "      <td>0.782923</td>\n",
              "      <td>-0.367487</td>\n",
              "      <td>-0.570916</td>\n",
              "      <td>0.209012</td>\n",
              "      <td>-0.314917</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270830</th>\n",
              "      <td>164278.0</td>\n",
              "      <td>-0.666107</td>\n",
              "      <td>1.955806</td>\n",
              "      <td>-0.970569</td>\n",
              "      <td>0.994797</td>\n",
              "      <td>1.430764</td>\n",
              "      <td>-0.223036</td>\n",
              "      <td>0.992730</td>\n",
              "      <td>-1.818298</td>\n",
              "      <td>-0.668780</td>\n",
              "      <td>-0.494698</td>\n",
              "      <td>-0.680511</td>\n",
              "      <td>-0.036268</td>\n",
              "      <td>0.238055</td>\n",
              "      <td>-1.103027</td>\n",
              "      <td>-0.197782</td>\n",
              "      <td>-0.667978</td>\n",
              "      <td>1.373538</td>\n",
              "      <td>0.318554</td>\n",
              "      <td>0.558223</td>\n",
              "      <td>-0.217497</td>\n",
              "      <td>1.528994</td>\n",
              "      <td>-0.117638</td>\n",
              "      <td>-0.144217</td>\n",
              "      <td>0.486151</td>\n",
              "      <td>0.109667</td>\n",
              "      <td>-0.388068</td>\n",
              "      <td>0.569650</td>\n",
              "      <td>0.281173</td>\n",
              "      <td>35.19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281438</th>\n",
              "      <td>170182.0</td>\n",
              "      <td>-2.975257</td>\n",
              "      <td>3.082269</td>\n",
              "      <td>-3.426893</td>\n",
              "      <td>-2.956799</td>\n",
              "      <td>3.093995</td>\n",
              "      <td>2.596861</td>\n",
              "      <td>1.217079</td>\n",
              "      <td>0.518896</td>\n",
              "      <td>2.189580</td>\n",
              "      <td>3.950829</td>\n",
              "      <td>0.268061</td>\n",
              "      <td>-0.118363</td>\n",
              "      <td>-0.505003</td>\n",
              "      <td>-0.295380</td>\n",
              "      <td>0.486345</td>\n",
              "      <td>-1.138985</td>\n",
              "      <td>-0.741088</td>\n",
              "      <td>-0.752800</td>\n",
              "      <td>-0.648189</td>\n",
              "      <td>1.777793</td>\n",
              "      <td>-0.255157</td>\n",
              "      <td>0.581680</td>\n",
              "      <td>-0.111795</td>\n",
              "      <td>0.700873</td>\n",
              "      <td>0.343542</td>\n",
              "      <td>0.084868</td>\n",
              "      <td>1.153941</td>\n",
              "      <td>0.344009</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83897</th>\n",
              "      <td>60075.0</td>\n",
              "      <td>1.446924</td>\n",
              "      <td>-0.427289</td>\n",
              "      <td>-0.643136</td>\n",
              "      <td>-0.855597</td>\n",
              "      <td>-0.195053</td>\n",
              "      <td>-0.831853</td>\n",
              "      <td>-0.000067</td>\n",
              "      <td>-0.357523</td>\n",
              "      <td>-1.091096</td>\n",
              "      <td>0.670830</td>\n",
              "      <td>-1.095000</td>\n",
              "      <td>-0.933351</td>\n",
              "      <td>0.299323</td>\n",
              "      <td>0.064001</td>\n",
              "      <td>0.498135</td>\n",
              "      <td>1.014239</td>\n",
              "      <td>0.055914</td>\n",
              "      <td>-1.277686</td>\n",
              "      <td>0.989231</td>\n",
              "      <td>0.170866</td>\n",
              "      <td>0.148607</td>\n",
              "      <td>0.286473</td>\n",
              "      <td>-0.315872</td>\n",
              "      <td>-0.528606</td>\n",
              "      <td>0.909388</td>\n",
              "      <td>-0.029130</td>\n",
              "      <td>-0.037010</td>\n",
              "      <td>-0.001473</td>\n",
              "      <td>50.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16768</th>\n",
              "      <td>28130.0</td>\n",
              "      <td>-5.656755</td>\n",
              "      <td>-0.689211</td>\n",
              "      <td>-1.200654</td>\n",
              "      <td>-0.226849</td>\n",
              "      <td>-1.690167</td>\n",
              "      <td>-0.368161</td>\n",
              "      <td>-1.867186</td>\n",
              "      <td>2.119909</td>\n",
              "      <td>1.110210</td>\n",
              "      <td>-0.784590</td>\n",
              "      <td>-1.855929</td>\n",
              "      <td>1.045463</td>\n",
              "      <td>0.164980</td>\n",
              "      <td>0.588652</td>\n",
              "      <td>-0.478331</td>\n",
              "      <td>0.847000</td>\n",
              "      <td>0.686102</td>\n",
              "      <td>-0.842290</td>\n",
              "      <td>-0.484885</td>\n",
              "      <td>-1.159874</td>\n",
              "      <td>-0.424794</td>\n",
              "      <td>-0.806220</td>\n",
              "      <td>-1.706960</td>\n",
              "      <td>-0.228396</td>\n",
              "      <td>-0.171114</td>\n",
              "      <td>0.835452</td>\n",
              "      <td>0.260218</td>\n",
              "      <td>-0.835033</td>\n",
              "      <td>154.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194503</th>\n",
              "      <td>130615.0</td>\n",
              "      <td>2.025547</td>\n",
              "      <td>-0.244872</td>\n",
              "      <td>-1.994590</td>\n",
              "      <td>0.344966</td>\n",
              "      <td>0.423622</td>\n",
              "      <td>-0.967608</td>\n",
              "      <td>0.536235</td>\n",
              "      <td>-0.402921</td>\n",
              "      <td>0.510157</td>\n",
              "      <td>0.099613</td>\n",
              "      <td>-1.653141</td>\n",
              "      <td>-0.197550</td>\n",
              "      <td>-0.544701</td>\n",
              "      <td>0.492695</td>\n",
              "      <td>-0.323184</td>\n",
              "      <td>-0.316073</td>\n",
              "      <td>-0.310110</td>\n",
              "      <td>-0.521732</td>\n",
              "      <td>0.466370</td>\n",
              "      <td>-0.128266</td>\n",
              "      <td>-0.054734</td>\n",
              "      <td>-0.100243</td>\n",
              "      <td>-0.061748</td>\n",
              "      <td>-0.666454</td>\n",
              "      <td>0.238047</td>\n",
              "      <td>0.600837</td>\n",
              "      <td>-0.112694</td>\n",
              "      <td>-0.075137</td>\n",
              "      <td>64.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>284807 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time        V1        V2  ...       V28  Amount  Class\n",
              "64185    51067.0  0.536717  0.090463  ... -0.122334   20.78      0\n",
              "36436    38535.0 -1.185253  0.397230  ...  0.009247   88.97      0\n",
              "274201  165899.0  1.866748  0.011582  ... -0.051858   74.86      0\n",
              "94663    64978.0 -0.397299  0.811866  ...  0.009893   89.55      0\n",
              "224225  143701.0 -1.087965  0.309144  ... -0.314917    5.00      0\n",
              "...          ...       ...       ...  ...       ...     ...    ...\n",
              "270830  164278.0 -0.666107  1.955806  ...  0.281173   35.19      0\n",
              "281438  170182.0 -2.975257  3.082269  ...  0.344009    7.70      0\n",
              "83897    60075.0  1.446924 -0.427289  ... -0.001473   50.00      0\n",
              "16768    28130.0 -5.656755 -0.689211  ... -0.835033  154.00      0\n",
              "194503  130615.0  2.025547 -0.244872  ... -0.075137   64.99      0\n",
              "\n",
              "[284807 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5HhgjC-gQMb"
      },
      "source": [
        "def df_lookslike(v_df):\n",
        "    #---------------------------------------How dataframe looks like?\n",
        "    v_df.info()\n",
        "    print(v_df.head(5))\n",
        "    total_cells=np.product(v_df.shape)\n",
        "    num_col = [i for i in v_df.columns if (v_df[i].dtype=='int64' or v_df[i].dtype=='float64')]\n",
        "    print(v_df[num_col].describe().loc[['min','max', 'mean','50%'],:]) #How big is Messy data?\n",
        "    missing_Values=v_df.isnull().sum()\n",
        "    print(missing_Values)\n",
        "    total_missing=missing_Values.sum()\n",
        "\n",
        "    #Percent of Missing data\n",
        "    print(\"Percent of data is missing:\",((total_missing/total_cells) * 100))\n",
        "    print(df[df.Time.isnull() == True ])\n",
        "    print(df[df.Time.isna() == True ])\n",
        "\n",
        "#\n",
        "df=df1\n",
        "    "
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG5RiE4MheL9"
      },
      "source": [
        "**2. Check Missing Values**\n",
        "( If Exist ; Fill each record with mean of its feature )\n",
        "\n",
        "I found there is NO missing/NULL data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4l7rF1NgQMb",
        "outputId": "9714d8a3-b8b9-4505-ac9f-1f939e7eed64"
      },
      "source": [
        "df_lookslike(df)# How DF looks like?"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 284807 entries, 64185 to 194503\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 69.5 MB\n",
            "            Time        V1        V2  ...       V28  Amount  Class\n",
            "64185    51067.0  0.536717  0.090463  ... -0.122334   20.78      0\n",
            "36436    38535.0 -1.185253  0.397230  ...  0.009247   88.97      0\n",
            "274201  165899.0  1.866748  0.011582  ... -0.051858   74.86      0\n",
            "94663    64978.0 -0.397299  0.811866  ...  0.009893   89.55      0\n",
            "224225  143701.0 -1.087965  0.309144  ... -0.314917    5.00      0\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "               Time            V1  ...        Amount     Class\n",
            "min        0.000000 -5.640751e+01  ...      0.000000  0.000000\n",
            "max   172792.000000  2.454930e+00  ...  25691.160000  1.000000\n",
            "mean   94813.859575  1.127876e-15  ...     88.349619  0.001727\n",
            "50%    84692.000000  1.810880e-02  ...     22.000000  0.000000\n",
            "\n",
            "[4 rows x 31 columns]\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "Percent of data is missing: 0.0\n",
            "Empty DataFrame\n",
            "Columns: [Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class]\n",
            "Index: []\n",
            "Empty DataFrame\n",
            "Columns: [Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAj0lfgUiX_Z"
      },
      "source": [
        "**3. Split data**\n",
        "50% Training, 30% Test, and 20% Validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWUWtQi7gQMc",
        "outputId": "6f507e7a-5b83-4550-dd82-d28d9e8ad4b3"
      },
      "source": [
        "#The ideal situation is 1st SPLIT DATA then do the Normalization.\n",
        "# If we do Normalization before the Data splitting then TEST/VALIDATION Data will also be exposed w.r.t to MEAN & STD.\n",
        "#Which is absolutly NOT required/incorrect approach.\n",
        "\n",
        "#For picture data Normalization:\tWe divide each value by its Higher pixel value (for colored pic 255) \n",
        "#For Discrete/number data Normalization:\tWe extract mean and std\n",
        "\n",
        "#We can use following libraries to Normalize particular data.\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y=(df.loc[:,df.columns=='Class']) #Lets take Dependent Variable/Target in a serpate df i.e X.\n",
        "X=(df.loc[:,df.columns!='Class']) #Lets take Independent Variables in a serpate df i.e Y.\n",
        "\n",
        "print(\"How Y looks like:\\n\", Y.head(5))\n",
        "print(\"How Features/X looks like:\\n\", X.head(5))\n",
        "\n",
        "\n",
        "\n",
        "x_train_50,X_remaining,y_train_50,Y_remaining=train_test_split(X,Y,test_size=0.5,random_state=0)\n",
        "x_test_30,x_valid_20,y_test_30,y_valid_20=train_test_split(X_remaining,Y_remaining,test_size=0.7,random_state=0)\n",
        "\n",
        "print(\"X/Training Data shape [50%]:\\t\", x_train_50.shape)\n",
        "print(\"X/Test Data [30%]:\\t\", x_test_30.shape)\n",
        "print(\"X/Validate Data [20%]:\\t\", x_valid_20.shape)\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How Y looks like:\n",
            "         Class\n",
            "64185       0\n",
            "36436       0\n",
            "274201      0\n",
            "94663       0\n",
            "224225      0\n",
            "How Features/X looks like:\n",
            "             Time        V1        V2  ...       V27       V28  Amount\n",
            "64185    51067.0  0.536717  0.090463  ... -0.068988 -0.122334   20.78\n",
            "36436    38535.0 -1.185253  0.397230  ... -0.066212  0.009247   88.97\n",
            "274201  165899.0  1.866748  0.011582  ... -0.031393 -0.051858   74.86\n",
            "94663    64978.0 -0.397299  0.811866  ...  0.222827  0.009893   89.55\n",
            "224225  143701.0 -1.087965  0.309144  ...  0.209012 -0.314917    5.00\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "X/Training Data shape [50%]:\t (142403, 30)\n",
            "X/Test Data [30%]:\t (42721, 30)\n",
            "X/Validate Data [20%]:\t (99683, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNotSjw8jukh"
      },
      "source": [
        "**4. Data Normalization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R82MPVVUj-8-",
        "outputId": "45edb174-7110-458e-df10-1e2b4bb96706"
      },
      "source": [
        "#The ideal situation is 1st SPLIT DATA then do the Normalization.\n",
        "# If we do Normalization before the Data splitting then TEST/VALIDATION Data will also be exposed w.r.t to MEAN & STD.\n",
        "#Which is absolutly NOT required/incorrect approach.\n",
        "\n",
        "#For picture data Normalization:\tWe divide each value by its Higher pixel value (for colored pic 255) \n",
        "#For Discrete/number data Normalization:\tWe extract mean and std\n",
        "\n",
        "#We can use following libraries to Normalize particular data.\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "#Analyzing how DF looks like in pre-normalization phase.\n",
        "\n",
        "print(\"Pre Normalization X Training data:\\n\",x_train_50.head(5))\n",
        "print(\"Pre Normalization X Test data:\\n\",x_test_30.head(5))\n",
        "print(\"Pre Normalization X Valid data:\\n\",x_valid_20.head(5))\n",
        "\n",
        "#I'v checked there are two columns who's data is too high/low. Thus, Normalizing following TWO columns in individaul DF (Training, Test, Validation).\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "x_train_50[['Time', 'Amount']] = sc.fit_transform(x_train_50[['Time', 'Amount']])\n",
        "x_test_30[['Time', 'Amount']] = sc.fit_transform(x_test_30[['Time', 'Amount']])\n",
        "x_valid_20[['Time', 'Amount']] = sc.fit_transform(x_valid_20[['Time', 'Amount']])\n",
        "\n",
        "print(\"Post Normalization X Training data:\\n\",x_train_50.head(5))\n",
        "print(\"Post Normalization X Test data:\\n\",x_test_30.head(5))\n",
        "print(\"Post Normalization X Valid data:\\n\",x_valid_20.head(5))\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre Normalization X Training data:\n",
            "             Time        V1        V2  ...       V27       V28  Amount\n",
            "70893    54063.0  1.236275  0.232268  ... -0.032976  0.015403    1.78\n",
            "206104  136069.0  1.092706 -0.841944  ... -0.068124  0.027253  376.93\n",
            "244971  152554.0  1.322497 -1.574789  ... -0.100619  0.005266  356.00\n",
            "284099  172150.0  2.080279  0.216415  ... -0.057459 -0.029361    1.29\n",
            "261915  160238.0  2.152922  0.075237  ... -0.062596 -0.079286   18.96\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Pre Normalization X Test data:\n",
            "             Time        V1        V2  ...       V27       V28  Amount\n",
            "271221  164469.0 -0.550971  0.044051  ...  0.211092  0.008349    2.00\n",
            "60665    49434.0  0.863584 -1.435374  ...  0.038830  0.065134  223.36\n",
            "262818  160658.0  2.194681 -1.376135  ...  0.033170 -0.078661   15.00\n",
            "24770    33375.0 -0.829061  0.344202  ... -0.086650  0.085158    1.02\n",
            "86194    61127.0 -0.857244 -0.065917  ... -0.181520 -0.094433    1.00\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Pre Normalization X Valid data:\n",
            "             Time         V1        V2  ...       V27       V28  Amount\n",
            "43159    41397.0 -13.584433 -9.002633  ... -2.732322 -0.314129  436.00\n",
            "18399    29459.0   1.052066 -0.221035  ...  0.019792 -0.015309    1.79\n",
            "211627  138489.0  -2.587498 -0.295363  ... -0.490349  0.055799   11.66\n",
            "194421  130575.0   0.567631 -3.685937  ... -0.022091  0.071316  719.50\n",
            "57317    47856.0  -0.906682 -0.694438  ...  0.005344  0.073421  134.99\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Post Normalization X Training data:\n",
            "             Time        V1        V2  ...       V27       V28    Amount\n",
            "70893  -0.860405  1.236275  0.232268  ... -0.032976  0.015403 -0.355022\n",
            "206104  0.868101  1.092706 -0.841944  ... -0.068124  0.027253  1.184920\n",
            "244971  1.215568  1.322497 -1.574789  ... -0.100619  0.005266  1.099005\n",
            "284099  1.628608  2.080279  0.216415  ... -0.057459 -0.029361 -0.357033\n",
            "261915  1.377530  2.152922  0.075237  ... -0.062596 -0.079286 -0.284500\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Post Normalization X Test data:\n",
            "             Time        V1        V2  ...       V27       V28    Amount\n",
            "271221  1.459334 -0.550971  0.044051  ...  0.211092  0.008349 -0.358785\n",
            "60665  -0.958079  0.863584 -1.435374  ...  0.038830  0.065134  0.579422\n",
            "262818  1.379247  2.194681 -1.376135  ...  0.033170 -0.078661 -0.303686\n",
            "24770  -1.295552 -0.829061  0.344202  ... -0.086650  0.085158 -0.362938\n",
            "86194  -0.712355 -0.857244 -0.065917  ... -0.181520 -0.094433 -0.363023\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Post Normalization X Valid data:\n",
            "             Time         V1        V2  ...       V27       V28    Amount\n",
            "43159  -1.120347 -13.584433 -9.002633  ... -2.732322 -0.314129  1.309858\n",
            "18399  -1.371625   1.052066 -0.221035  ...  0.019792 -0.015309 -0.330118\n",
            "211627  0.923295  -2.587498 -0.295363  ... -0.490349  0.055799 -0.292840\n",
            "194421  0.756717   0.567631 -3.685937  ... -0.022091  0.071316  2.380615\n",
            "57317  -0.984395  -0.906682 -0.694438  ...  0.005344  0.073421  0.172968\n",
            "\n",
            "[5 rows x 30 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3076: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.iloc._setitem_with_indexer((slice(None), indexer), value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3041: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_array(key, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3076: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.iloc._setitem_with_indexer((slice(None), indexer), value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3041: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_array(key, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3076: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.iloc._setitem_with_indexer((slice(None), indexer), value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3041: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_array(key, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ3eDu2miyT9"
      },
      "source": [
        "**5.Model :**\n",
        "Input Layer (No. of features ), 4 hidden layers including 10,8,6 unit & Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_q6_Sdhj-P-"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "network = Sequential()\n",
        "\n",
        "network.add(layers.Dense(10, activation='relu', kernel_regularizer =regularizers.l2(0.02),   input_shape=(x_train_50.shape[1],)))\n",
        "network.add(layers.Dense(8, activation='relu', kernel_regularizer =regularizers.l2(0.02)))\n",
        "#I'm passing 01 Neurons and Sigmoid/probability as Activation function as as output is binary.\n",
        "#However, for Regression also use 01-Neurons but do not specify Activation function.\n",
        "network.add(layers.Dense(1, activation='sigmoid', kernel_regularizer =regularizers.l2(0.002)))\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-u1LCxWi485"
      },
      "source": [
        "**6.Compilation Step**\n",
        "(Note : Its a Binary problem , select loss , metrics according to it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kl4hmTSgQMd"
      },
      "source": [
        "from tensorflow import keras\n",
        "#Preparing parameters for Optimizer.\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01) #I want lowest learning rate as higher accuracy required.\n",
        "network.compile(optimizer=opt, loss='binary_crossentropy', metrics='accuracy')"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sw4YdYZi9zs"
      },
      "source": [
        "**7.Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx1_GRjojKat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e5d24f-0fec-4630-eee4-2fe3fa2f1ac3"
      },
      "source": [
        "model_history = network.fit(x_train_50,y_train_50, batch_size=1024, epochs=5, validation_data=(x_valid_20, y_valid_20))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.4133 - accuracy: 0.9580 - val_loss: 0.0296 - val_accuracy: 0.9982\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.9985 - val_loss: 0.0211 - val_accuracy: 0.9982\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.9984 - val_loss: 0.0199 - val_accuracy: 0.9982\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9986 - val_loss: 0.0186 - val_accuracy: 0.9982\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.9983 - val_loss: 0.0179 - val_accuracy: 0.9982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK75Q1B1jLFb"
      },
      "source": [
        "Training_Loss = model_history.history['loss']\n",
        "Validated_Loss = model_history.history['val_loss']\n",
        "Each_Epochs=range(1,6)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tGPYizaYi6YK",
        "outputId": "c3bf9c6d-0e7e-4359-e2f2-00b6f5240f99"
      },
      "source": [
        "plt.plot(Each_Epochs, Training_Loss, 'r', label='Training Loss')\n",
        "plt.plot(Each_Epochs, Validated_Loss, 'g', label='Validation Loss')\n",
        "plt.title('Training vs Validation Losses ')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXo73FTbjFxq"
      },
      "source": [
        "**8.Prediction** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNBvakoLk30x",
        "outputId": "6c56ef5d-4b46-4295-d1f7-f8f31715bc08"
      },
      "source": [
        "network.predict(x_test_30)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00228611],\n",
              "       [0.00232324],\n",
              "       [0.00228024],\n",
              "       ...,\n",
              "       [0.0023661 ],\n",
              "       [0.0023098 ],\n",
              "       [0.00233018]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg8NQOyDk34y",
        "outputId": "90e0e960-9cb9-4989-a542-0b6812efa19c"
      },
      "source": [
        "network.evaluate(x_test_30,y_test_30)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1336/1336 [==============================] - 1s 1ms/step - loss: 0.0190 - accuracy: 0.9980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01896653138101101, 0.9980337619781494]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1AFTdSTlPnE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}