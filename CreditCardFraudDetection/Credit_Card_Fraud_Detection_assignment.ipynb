{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Credit_Card_Fraud_Detection_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DcIIUidkgQMR",
        "-aVY0NfJgQMY"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcIIUidkgQMR"
      },
      "source": [
        "# Credit Card Fraud Detection::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoUbY5SZgQMX"
      },
      "source": [
        "Download dataset from this link:\n",
        "\n",
        "https://www.kaggle.com/mlg-ulb/creditcardfraud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aVY0NfJgQMY"
      },
      "source": [
        "# Description about dataset::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5JKjSpngQMY"
      },
      "source": [
        "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
        "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. \n",
        "\n",
        "\n",
        "### Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsfQk8KngQMZ"
      },
      "source": [
        "# WORKFLOW :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g97HlYVEgQMa"
      },
      "source": [
        "1.Load Data\n",
        "\n",
        "2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n",
        "\n",
        "3.Standardized the Input Variables. \n",
        "\n",
        "4.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n",
        "\n",
        "5.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).\n",
        "\n",
        "6.Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)\n",
        "\n",
        "7.Train the Model with Epochs (100).\n",
        "\n",
        "8.If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .\n",
        "\n",
        "9.Prediction should be > 92%\n",
        "10.Evaluation Step\n",
        "11Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdDEsgaSjD6V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMA7kqgkgQMa"
      },
      "source": [
        "# Task::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBuW37DIgQMa"
      },
      "source": [
        "## Identify fraudulent credit card transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhD3UK-hmAN"
      },
      "source": [
        "**1. Load Data**\n",
        "Loading and immediately shuffeling Data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CsQJ-Z4RgQMa",
        "outputId": "7ca7880c-79e5-4b36-bdef-763ffff183a4"
      },
      "source": [
        "# Prediction Model developed by:\n",
        "# Khurram Nazir\n",
        "#  \n",
        "#\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "#\n",
        "\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_file = files.upload()\n",
        "#df = pd.read_csv(\"C:/Users/khurr/Documents/GitHub/AI-Engineering/PIAIC/Quarter-2/DeepLearning/CreditCardFraudDetection/creditcard.csv\",sep=',')\n",
        "df = pd.DataFrame(pd.read_csv(io.BytesIO(uploaded_file['creditcard.csv']),sep=','))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fa21f545-1c6a-4de2-b49f-f56da5b5391b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fa21f545-1c6a-4de2-b49f-f56da5b5391b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving creditcard.csv to creditcard.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qreMKOKSm491"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "UmL4NfbRgQMb",
        "outputId": "4fe922e5-8e80-4e40-d29e-6cdd050dcee0"
      },
      "source": [
        "df = df.sample(frac=1) #Shuffeling DF. \n",
        "df1=df #Aviding to reload data again & again from file.\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64185</th>\n",
              "      <td>51067.0</td>\n",
              "      <td>0.536717</td>\n",
              "      <td>0.090463</td>\n",
              "      <td>1.134899</td>\n",
              "      <td>1.724024</td>\n",
              "      <td>-0.635885</td>\n",
              "      <td>0.793883</td>\n",
              "      <td>-0.421369</td>\n",
              "      <td>0.428730</td>\n",
              "      <td>1.024854</td>\n",
              "      <td>0.020049</td>\n",
              "      <td>0.664470</td>\n",
              "      <td>1.551671</td>\n",
              "      <td>-0.445985</td>\n",
              "      <td>-0.680214</td>\n",
              "      <td>-2.402238</td>\n",
              "      <td>-1.206342</td>\n",
              "      <td>0.709892</td>\n",
              "      <td>-0.741002</td>\n",
              "      <td>0.849804</td>\n",
              "      <td>-0.236664</td>\n",
              "      <td>-0.225364</td>\n",
              "      <td>-0.208870</td>\n",
              "      <td>0.412062</td>\n",
              "      <td>0.199573</td>\n",
              "      <td>-1.018516</td>\n",
              "      <td>-0.675466</td>\n",
              "      <td>-0.068988</td>\n",
              "      <td>-0.122334</td>\n",
              "      <td>20.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36436</th>\n",
              "      <td>38535.0</td>\n",
              "      <td>-1.185253</td>\n",
              "      <td>0.397230</td>\n",
              "      <td>0.636781</td>\n",
              "      <td>-2.139428</td>\n",
              "      <td>-0.454006</td>\n",
              "      <td>-0.224041</td>\n",
              "      <td>0.502780</td>\n",
              "      <td>0.603537</td>\n",
              "      <td>0.854961</td>\n",
              "      <td>-1.773599</td>\n",
              "      <td>0.458254</td>\n",
              "      <td>0.099457</td>\n",
              "      <td>-2.523987</td>\n",
              "      <td>0.965988</td>\n",
              "      <td>-0.131108</td>\n",
              "      <td>-0.365195</td>\n",
              "      <td>-0.119036</td>\n",
              "      <td>0.229618</td>\n",
              "      <td>0.097433</td>\n",
              "      <td>-0.489503</td>\n",
              "      <td>-0.015689</td>\n",
              "      <td>-0.144561</td>\n",
              "      <td>0.060228</td>\n",
              "      <td>-0.377304</td>\n",
              "      <td>0.016670</td>\n",
              "      <td>-1.005483</td>\n",
              "      <td>-0.066212</td>\n",
              "      <td>0.009247</td>\n",
              "      <td>88.97</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274201</th>\n",
              "      <td>165899.0</td>\n",
              "      <td>1.866748</td>\n",
              "      <td>0.011582</td>\n",
              "      <td>-1.781247</td>\n",
              "      <td>1.215596</td>\n",
              "      <td>0.730180</td>\n",
              "      <td>-0.378462</td>\n",
              "      <td>0.619591</td>\n",
              "      <td>-0.234691</td>\n",
              "      <td>-0.185538</td>\n",
              "      <td>0.368097</td>\n",
              "      <td>0.851455</td>\n",
              "      <td>1.352516</td>\n",
              "      <td>0.376523</td>\n",
              "      <td>0.593604</td>\n",
              "      <td>-1.244699</td>\n",
              "      <td>-0.546856</td>\n",
              "      <td>-0.430146</td>\n",
              "      <td>-0.209207</td>\n",
              "      <td>0.042203</td>\n",
              "      <td>-0.081348</td>\n",
              "      <td>0.145675</td>\n",
              "      <td>0.456557</td>\n",
              "      <td>-0.041958</td>\n",
              "      <td>0.731332</td>\n",
              "      <td>0.442545</td>\n",
              "      <td>-0.552150</td>\n",
              "      <td>-0.031393</td>\n",
              "      <td>-0.051858</td>\n",
              "      <td>74.86</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94663</th>\n",
              "      <td>64978.0</td>\n",
              "      <td>-0.397299</td>\n",
              "      <td>0.811866</td>\n",
              "      <td>1.284498</td>\n",
              "      <td>0.879811</td>\n",
              "      <td>0.076744</td>\n",
              "      <td>-0.665860</td>\n",
              "      <td>1.250132</td>\n",
              "      <td>-0.362702</td>\n",
              "      <td>-0.419350</td>\n",
              "      <td>0.123263</td>\n",
              "      <td>-0.428271</td>\n",
              "      <td>-0.231698</td>\n",
              "      <td>0.004358</td>\n",
              "      <td>0.113895</td>\n",
              "      <td>1.057510</td>\n",
              "      <td>-0.400089</td>\n",
              "      <td>-0.300428</td>\n",
              "      <td>0.111855</td>\n",
              "      <td>0.385011</td>\n",
              "      <td>0.403862</td>\n",
              "      <td>0.111049</td>\n",
              "      <td>0.505111</td>\n",
              "      <td>0.005072</td>\n",
              "      <td>0.384240</td>\n",
              "      <td>-0.183635</td>\n",
              "      <td>-0.348163</td>\n",
              "      <td>0.222827</td>\n",
              "      <td>0.009893</td>\n",
              "      <td>89.55</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224225</th>\n",
              "      <td>143701.0</td>\n",
              "      <td>-1.087965</td>\n",
              "      <td>0.309144</td>\n",
              "      <td>-1.424678</td>\n",
              "      <td>-0.002609</td>\n",
              "      <td>0.867860</td>\n",
              "      <td>-0.949465</td>\n",
              "      <td>-0.252551</td>\n",
              "      <td>-0.497153</td>\n",
              "      <td>0.209420</td>\n",
              "      <td>0.138327</td>\n",
              "      <td>0.527635</td>\n",
              "      <td>0.389882</td>\n",
              "      <td>-1.348132</td>\n",
              "      <td>1.208748</td>\n",
              "      <td>-0.077758</td>\n",
              "      <td>-0.436654</td>\n",
              "      <td>-0.127048</td>\n",
              "      <td>0.575521</td>\n",
              "      <td>0.878454</td>\n",
              "      <td>-0.633939</td>\n",
              "      <td>0.932414</td>\n",
              "      <td>0.323533</td>\n",
              "      <td>-0.441924</td>\n",
              "      <td>0.782923</td>\n",
              "      <td>-0.367487</td>\n",
              "      <td>-0.570916</td>\n",
              "      <td>0.209012</td>\n",
              "      <td>-0.314917</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270830</th>\n",
              "      <td>164278.0</td>\n",
              "      <td>-0.666107</td>\n",
              "      <td>1.955806</td>\n",
              "      <td>-0.970569</td>\n",
              "      <td>0.994797</td>\n",
              "      <td>1.430764</td>\n",
              "      <td>-0.223036</td>\n",
              "      <td>0.992730</td>\n",
              "      <td>-1.818298</td>\n",
              "      <td>-0.668780</td>\n",
              "      <td>-0.494698</td>\n",
              "      <td>-0.680511</td>\n",
              "      <td>-0.036268</td>\n",
              "      <td>0.238055</td>\n",
              "      <td>-1.103027</td>\n",
              "      <td>-0.197782</td>\n",
              "      <td>-0.667978</td>\n",
              "      <td>1.373538</td>\n",
              "      <td>0.318554</td>\n",
              "      <td>0.558223</td>\n",
              "      <td>-0.217497</td>\n",
              "      <td>1.528994</td>\n",
              "      <td>-0.117638</td>\n",
              "      <td>-0.144217</td>\n",
              "      <td>0.486151</td>\n",
              "      <td>0.109667</td>\n",
              "      <td>-0.388068</td>\n",
              "      <td>0.569650</td>\n",
              "      <td>0.281173</td>\n",
              "      <td>35.19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281438</th>\n",
              "      <td>170182.0</td>\n",
              "      <td>-2.975257</td>\n",
              "      <td>3.082269</td>\n",
              "      <td>-3.426893</td>\n",
              "      <td>-2.956799</td>\n",
              "      <td>3.093995</td>\n",
              "      <td>2.596861</td>\n",
              "      <td>1.217079</td>\n",
              "      <td>0.518896</td>\n",
              "      <td>2.189580</td>\n",
              "      <td>3.950829</td>\n",
              "      <td>0.268061</td>\n",
              "      <td>-0.118363</td>\n",
              "      <td>-0.505003</td>\n",
              "      <td>-0.295380</td>\n",
              "      <td>0.486345</td>\n",
              "      <td>-1.138985</td>\n",
              "      <td>-0.741088</td>\n",
              "      <td>-0.752800</td>\n",
              "      <td>-0.648189</td>\n",
              "      <td>1.777793</td>\n",
              "      <td>-0.255157</td>\n",
              "      <td>0.581680</td>\n",
              "      <td>-0.111795</td>\n",
              "      <td>0.700873</td>\n",
              "      <td>0.343542</td>\n",
              "      <td>0.084868</td>\n",
              "      <td>1.153941</td>\n",
              "      <td>0.344009</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83897</th>\n",
              "      <td>60075.0</td>\n",
              "      <td>1.446924</td>\n",
              "      <td>-0.427289</td>\n",
              "      <td>-0.643136</td>\n",
              "      <td>-0.855597</td>\n",
              "      <td>-0.195053</td>\n",
              "      <td>-0.831853</td>\n",
              "      <td>-0.000067</td>\n",
              "      <td>-0.357523</td>\n",
              "      <td>-1.091096</td>\n",
              "      <td>0.670830</td>\n",
              "      <td>-1.095000</td>\n",
              "      <td>-0.933351</td>\n",
              "      <td>0.299323</td>\n",
              "      <td>0.064001</td>\n",
              "      <td>0.498135</td>\n",
              "      <td>1.014239</td>\n",
              "      <td>0.055914</td>\n",
              "      <td>-1.277686</td>\n",
              "      <td>0.989231</td>\n",
              "      <td>0.170866</td>\n",
              "      <td>0.148607</td>\n",
              "      <td>0.286473</td>\n",
              "      <td>-0.315872</td>\n",
              "      <td>-0.528606</td>\n",
              "      <td>0.909388</td>\n",
              "      <td>-0.029130</td>\n",
              "      <td>-0.037010</td>\n",
              "      <td>-0.001473</td>\n",
              "      <td>50.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16768</th>\n",
              "      <td>28130.0</td>\n",
              "      <td>-5.656755</td>\n",
              "      <td>-0.689211</td>\n",
              "      <td>-1.200654</td>\n",
              "      <td>-0.226849</td>\n",
              "      <td>-1.690167</td>\n",
              "      <td>-0.368161</td>\n",
              "      <td>-1.867186</td>\n",
              "      <td>2.119909</td>\n",
              "      <td>1.110210</td>\n",
              "      <td>-0.784590</td>\n",
              "      <td>-1.855929</td>\n",
              "      <td>1.045463</td>\n",
              "      <td>0.164980</td>\n",
              "      <td>0.588652</td>\n",
              "      <td>-0.478331</td>\n",
              "      <td>0.847000</td>\n",
              "      <td>0.686102</td>\n",
              "      <td>-0.842290</td>\n",
              "      <td>-0.484885</td>\n",
              "      <td>-1.159874</td>\n",
              "      <td>-0.424794</td>\n",
              "      <td>-0.806220</td>\n",
              "      <td>-1.706960</td>\n",
              "      <td>-0.228396</td>\n",
              "      <td>-0.171114</td>\n",
              "      <td>0.835452</td>\n",
              "      <td>0.260218</td>\n",
              "      <td>-0.835033</td>\n",
              "      <td>154.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194503</th>\n",
              "      <td>130615.0</td>\n",
              "      <td>2.025547</td>\n",
              "      <td>-0.244872</td>\n",
              "      <td>-1.994590</td>\n",
              "      <td>0.344966</td>\n",
              "      <td>0.423622</td>\n",
              "      <td>-0.967608</td>\n",
              "      <td>0.536235</td>\n",
              "      <td>-0.402921</td>\n",
              "      <td>0.510157</td>\n",
              "      <td>0.099613</td>\n",
              "      <td>-1.653141</td>\n",
              "      <td>-0.197550</td>\n",
              "      <td>-0.544701</td>\n",
              "      <td>0.492695</td>\n",
              "      <td>-0.323184</td>\n",
              "      <td>-0.316073</td>\n",
              "      <td>-0.310110</td>\n",
              "      <td>-0.521732</td>\n",
              "      <td>0.466370</td>\n",
              "      <td>-0.128266</td>\n",
              "      <td>-0.054734</td>\n",
              "      <td>-0.100243</td>\n",
              "      <td>-0.061748</td>\n",
              "      <td>-0.666454</td>\n",
              "      <td>0.238047</td>\n",
              "      <td>0.600837</td>\n",
              "      <td>-0.112694</td>\n",
              "      <td>-0.075137</td>\n",
              "      <td>64.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>284807 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time        V1        V2  ...       V28  Amount  Class\n",
              "64185    51067.0  0.536717  0.090463  ... -0.122334   20.78      0\n",
              "36436    38535.0 -1.185253  0.397230  ...  0.009247   88.97      0\n",
              "274201  165899.0  1.866748  0.011582  ... -0.051858   74.86      0\n",
              "94663    64978.0 -0.397299  0.811866  ...  0.009893   89.55      0\n",
              "224225  143701.0 -1.087965  0.309144  ... -0.314917    5.00      0\n",
              "...          ...       ...       ...  ...       ...     ...    ...\n",
              "270830  164278.0 -0.666107  1.955806  ...  0.281173   35.19      0\n",
              "281438  170182.0 -2.975257  3.082269  ...  0.344009    7.70      0\n",
              "83897    60075.0  1.446924 -0.427289  ... -0.001473   50.00      0\n",
              "16768    28130.0 -5.656755 -0.689211  ... -0.835033  154.00      0\n",
              "194503  130615.0  2.025547 -0.244872  ... -0.075137   64.99      0\n",
              "\n",
              "[284807 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5HhgjC-gQMb"
      },
      "source": [
        "def df_lookslike(v_df):\n",
        "    #---------------------------------------How dataframe looks like?\n",
        "    v_df.info()\n",
        "    print(v_df.head(5))\n",
        "    total_cells=np.product(v_df.shape)\n",
        "    num_col = [i for i in v_df.columns if (v_df[i].dtype=='int64' or v_df[i].dtype=='float64')]\n",
        "    print(v_df[num_col].describe().loc[['min','max', 'mean','50%'],:]) #How big is Messy data?\n",
        "    missing_Values=v_df.isnull().sum()\n",
        "    print(missing_Values)\n",
        "    total_missing=missing_Values.sum()\n",
        "\n",
        "    #Percent of Missing data\n",
        "    print(\"Percent of data is missing:\",((total_missing/total_cells) * 100))\n",
        "#\n",
        "df=df1\n",
        "    "
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG5RiE4MheL9"
      },
      "source": [
        "**2. Check Missing Values**\n",
        "( If Exist ; Fill each record with mean of its feature )\n",
        "\n",
        "I found there is NO missing/NULL data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4l7rF1NgQMb",
        "outputId": "4f1cd5f9-3bbc-40ee-9a18-b3ce1c16d76f"
      },
      "source": [
        "df_lookslike(df)# How DF looks like?"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 284807 entries, 64185 to 194503\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 69.5 MB\n",
            "            Time        V1        V2  ...       V28  Amount  Class\n",
            "64185    51067.0  0.536717  0.090463  ... -0.122334   20.78      0\n",
            "36436    38535.0 -1.185253  0.397230  ...  0.009247   88.97      0\n",
            "274201  165899.0  1.866748  0.011582  ... -0.051858   74.86      0\n",
            "94663    64978.0 -0.397299  0.811866  ...  0.009893   89.55      0\n",
            "224225  143701.0 -1.087965  0.309144  ... -0.314917    5.00      0\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "               Time            V1  ...        Amount     Class\n",
            "min        0.000000 -5.640751e+01  ...      0.000000  0.000000\n",
            "max   172792.000000  2.454930e+00  ...  25691.160000  1.000000\n",
            "mean   94813.859575  1.127876e-15  ...     88.349619  0.001727\n",
            "50%    84692.000000  1.810880e-02  ...     22.000000  0.000000\n",
            "\n",
            "[4 rows x 31 columns]\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "Percent of data is missing: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAj0lfgUiX_Z"
      },
      "source": [
        "**3. Split data**\n",
        "50% Training, 30% Test, and 20% Validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWUWtQi7gQMc",
        "outputId": "6a254ddb-bf14-4543-f14c-681e26a2cd68"
      },
      "source": [
        "#The ideal situation is 1st SPLIT DATA then do the Normalization.\n",
        "# If we do Normalization before the Data splitting then TEST/VALIDATION Data will also be exposed w.r.t to MEAN & STD.\n",
        "#Which is absolutly NOT required/incorrect approach.\n",
        "\n",
        "#For picture data Normalization:\tWe divide each value by its Higher pixel value (for colored pic 255)\n",
        "#For Discrete/number data Normalization:\tWe extract mean and std\n",
        "\n",
        "#We can use following libraries to Normalize particular data.\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "#However, in sklearn function shuffel parameter by defaul is True. But Remember Timeseries data shouldn't be shuffel :)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y=(df.loc[:,df.columns=='Class']) #Lets take Dependent Variable/Target in a serpate df i.e X.\n",
        "X=(df.loc[:,df.columns!='Class']) #Lets take Independent Variables in a serpate df i.e Y.\n",
        "\n",
        "print(\"How Y looks like:\\n\", Y.head(5))\n",
        "print(\"How Features/X looks like:\\n\", X.head(5))\n",
        "\n",
        "x_train_50,X_remaining,y_train_50,Y_remaining=train_test_split(X,Y,test_size=0.5,random_state=0)\n",
        "x_test_30,x_valid_20,y_test_30,y_valid_20=train_test_split(X_remaining,Y_remaining,test_size=0.7,random_state=0)\n",
        "\n",
        "print(\"X/Training Data shape [50%]:\\t\", x_train_50.shape)\n",
        "print(\"X/Test Data [30%]:\\t\", x_test_30.shape)\n",
        "print(\"X/Validate Data [20%]:\\t\", x_valid_20.shape)\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How Y looks like:\n",
            "         Class\n",
            "64185       0\n",
            "36436       0\n",
            "274201      0\n",
            "94663       0\n",
            "224225      0\n",
            "How Features/X looks like:\n",
            "             Time        V1        V2  ...       V27       V28  Amount\n",
            "64185    51067.0  0.536717  0.090463  ... -0.068988 -0.122334   20.78\n",
            "36436    38535.0 -1.185253  0.397230  ... -0.066212  0.009247   88.97\n",
            "274201  165899.0  1.866748  0.011582  ... -0.031393 -0.051858   74.86\n",
            "94663    64978.0 -0.397299  0.811866  ...  0.222827  0.009893   89.55\n",
            "224225  143701.0 -1.087965  0.309144  ...  0.209012 -0.314917    5.00\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "X/Training Data shape [50%]:\t (142403, 30)\n",
            "X/Test Data [30%]:\t (42721, 30)\n",
            "X/Validate Data [20%]:\t (99683, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNotSjw8jukh"
      },
      "source": [
        "**4. Data Normalization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R82MPVVUj-8-",
        "outputId": "3c96292a-fdea-4ce5-9eac-8c21fe2222b9"
      },
      "source": [
        "#The ideal situation is 1st SPLIT DATA then do the Normalization.\n",
        "# If we do Normalization before the Data splitting then TEST/VALIDATION Data will also be exposed w.r.t to MEAN & STD.\n",
        "#Which is absolutly NOT required/incorrect approach.\n",
        "\n",
        "#For picture data Normalization:\tWe divide each value by its Higher pixel value (for colored pic 255) \n",
        "#For Discrete/number data Normalization:\tWe extract mean and std\n",
        "\n",
        "#We can use following libraries to Normalize particular data.\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "#Analyzing how DF looks like in pre-normalization phase.\n",
        "\n",
        "print(\"Pre Normalization X Training data:\\n\",x_train_50.head(5))\n",
        "print(\"Pre Normalization X Test data:\\n\",x_test_30.head(5))\n",
        "print(\"Pre Normalization X Valid data:\\n\",x_valid_20.head(5))\n",
        "\n",
        "#I'v checked there are two columns who's data is too high/low. Thus, Normalizing following TWO columns in individaul DF (Training, Test, Validation).\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "x_train_50[['Time', 'Amount']] = sc.fit_transform(x_train_50[['Time', 'Amount']])\n",
        "x_test_30[['Time', 'Amount']] = sc.fit_transform(x_test_30[['Time', 'Amount']])\n",
        "x_valid_20[['Time', 'Amount']] = sc.fit_transform(x_valid_20[['Time', 'Amount']])\n",
        "\n",
        "print(\"Post Normalization X Training data:\\n\",x_train_50.head(5))\n",
        "print(\"Post Normalization X Test data:\\n\",x_test_30.head(5))\n",
        "print(\"Post Normalization X Valid data:\\n\",x_valid_20.head(5))\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre Normalization X Training data:\n",
            "             Time        V1        V2  ...       V27       V28  Amount\n",
            "70893    54063.0  1.236275  0.232268  ... -0.032976  0.015403    1.78\n",
            "206104  136069.0  1.092706 -0.841944  ... -0.068124  0.027253  376.93\n",
            "244971  152554.0  1.322497 -1.574789  ... -0.100619  0.005266  356.00\n",
            "284099  172150.0  2.080279  0.216415  ... -0.057459 -0.029361    1.29\n",
            "261915  160238.0  2.152922  0.075237  ... -0.062596 -0.079286   18.96\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Pre Normalization X Test data:\n",
            "             Time        V1        V2  ...       V27       V28  Amount\n",
            "271221  164469.0 -0.550971  0.044051  ...  0.211092  0.008349    2.00\n",
            "60665    49434.0  0.863584 -1.435374  ...  0.038830  0.065134  223.36\n",
            "262818  160658.0  2.194681 -1.376135  ...  0.033170 -0.078661   15.00\n",
            "24770    33375.0 -0.829061  0.344202  ... -0.086650  0.085158    1.02\n",
            "86194    61127.0 -0.857244 -0.065917  ... -0.181520 -0.094433    1.00\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Pre Normalization X Valid data:\n",
            "             Time         V1        V2  ...       V27       V28  Amount\n",
            "43159    41397.0 -13.584433 -9.002633  ... -2.732322 -0.314129  436.00\n",
            "18399    29459.0   1.052066 -0.221035  ...  0.019792 -0.015309    1.79\n",
            "211627  138489.0  -2.587498 -0.295363  ... -0.490349  0.055799   11.66\n",
            "194421  130575.0   0.567631 -3.685937  ... -0.022091  0.071316  719.50\n",
            "57317    47856.0  -0.906682 -0.694438  ...  0.005344  0.073421  134.99\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Post Normalization X Training data:\n",
            "             Time        V1        V2  ...       V27       V28    Amount\n",
            "70893  -0.860405  1.236275  0.232268  ... -0.032976  0.015403 -0.355022\n",
            "206104  0.868101  1.092706 -0.841944  ... -0.068124  0.027253  1.184920\n",
            "244971  1.215568  1.322497 -1.574789  ... -0.100619  0.005266  1.099005\n",
            "284099  1.628608  2.080279  0.216415  ... -0.057459 -0.029361 -0.357033\n",
            "261915  1.377530  2.152922  0.075237  ... -0.062596 -0.079286 -0.284500\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Post Normalization X Test data:\n",
            "             Time        V1        V2  ...       V27       V28    Amount\n",
            "271221  1.459334 -0.550971  0.044051  ...  0.211092  0.008349 -0.358785\n",
            "60665  -0.958079  0.863584 -1.435374  ...  0.038830  0.065134  0.579422\n",
            "262818  1.379247  2.194681 -1.376135  ...  0.033170 -0.078661 -0.303686\n",
            "24770  -1.295552 -0.829061  0.344202  ... -0.086650  0.085158 -0.362938\n",
            "86194  -0.712355 -0.857244 -0.065917  ... -0.181520 -0.094433 -0.363023\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Post Normalization X Valid data:\n",
            "             Time         V1        V2  ...       V27       V28    Amount\n",
            "43159  -1.120347 -13.584433 -9.002633  ... -2.732322 -0.314129  1.309858\n",
            "18399  -1.371625   1.052066 -0.221035  ...  0.019792 -0.015309 -0.330118\n",
            "211627  0.923295  -2.587498 -0.295363  ... -0.490349  0.055799 -0.292840\n",
            "194421  0.756717   0.567631 -3.685937  ... -0.022091  0.071316  2.380615\n",
            "57317  -0.984395  -0.906682 -0.694438  ...  0.005344  0.073421  0.172968\n",
            "\n",
            "[5 rows x 30 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3076: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.iloc._setitem_with_indexer((slice(None), indexer), value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3041: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_array(key, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3076: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.iloc._setitem_with_indexer((slice(None), indexer), value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3041: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_array(key, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3076: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.iloc._setitem_with_indexer((slice(None), indexer), value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3041: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_array(key, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ3eDu2miyT9"
      },
      "source": [
        "**5.Model :**\n",
        "Input Layer (No. of features ), 4 hidden layers including 10,8,6 unit & Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_q6_Sdhj-P-"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "network = Sequential()\n",
        "\n",
        "network.add(layers.Dense(10, activation='relu', kernel_regularizer =regularizers.l2(0.02),   input_shape=(x_train_50.shape[1],)))\n",
        "network.add(layers.Dense(8, activation='relu', kernel_regularizer =regularizers.l2(0.02)))\n",
        "#I'm passing 01 Neurons and Sigmoid/probability as Activation function as as output is binary.\n",
        "#However, for Regression also use 01-Neurons but do not specify Activation function.\n",
        "network.add(layers.Dense(1, activation='sigmoid', kernel_regularizer =regularizers.l2(0.002)))\n"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-u1LCxWi485"
      },
      "source": [
        "**6.Compilation Step**\n",
        "(Note : Its a Binary problem , select loss , metrics according to it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kl4hmTSgQMd"
      },
      "source": [
        "from tensorflow import keras\n",
        "#Preparing parameters for Optimizer.\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01) #I want lowest learning rate as higher accuracy required.\n",
        "network.compile(optimizer=opt, loss='binary_crossentropy', metrics='accuracy')"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sw4YdYZi9zs"
      },
      "source": [
        "**7.Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx1_GRjojKat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eae41cc-1443-4663-aeed-733919b6e142"
      },
      "source": [
        "model_history = network.fit(x_train_50,y_train_50, batch_size=1024, epochs=200, validation_data=(x_valid_20, y_valid_20))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "140/140 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9982\n",
            "Epoch 2/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.9984 - val_loss: 0.0242 - val_accuracy: 0.9982\n",
            "Epoch 3/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9983 - val_loss: 0.0217 - val_accuracy: 0.9982\n",
            "Epoch 4/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.9985 - val_loss: 0.0201 - val_accuracy: 0.9982\n",
            "Epoch 5/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.9984 - val_loss: 0.0191 - val_accuracy: 0.9982\n",
            "Epoch 6/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9985 - val_loss: 0.0184 - val_accuracy: 0.9982\n",
            "Epoch 7/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9984 - val_loss: 0.0177 - val_accuracy: 0.9982\n",
            "Epoch 8/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9983 - val_loss: 0.0173 - val_accuracy: 0.9982\n",
            "Epoch 9/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.9984 - val_loss: 0.0168 - val_accuracy: 0.9982\n",
            "Epoch 10/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 0.9983 - val_loss: 0.0165 - val_accuracy: 0.9982\n",
            "Epoch 11/200\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9982 - val_loss: 0.0166 - val_accuracy: 0.9982\n",
            "Epoch 12/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.9982 - val_loss: 0.0159 - val_accuracy: 0.9982\n",
            "Epoch 13/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9983 - val_loss: 0.0160 - val_accuracy: 0.9982\n",
            "Epoch 14/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.9982 - val_loss: 0.0157 - val_accuracy: 0.9982\n",
            "Epoch 15/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.0154 - val_accuracy: 0.9982\n",
            "Epoch 16/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9982 - val_loss: 0.0151 - val_accuracy: 0.9982\n",
            "Epoch 17/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9984 - val_loss: 0.0150 - val_accuracy: 0.9982\n",
            "Epoch 18/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9983 - val_loss: 0.0149 - val_accuracy: 0.9982\n",
            "Epoch 19/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9985 - val_loss: 0.0147 - val_accuracy: 0.9982\n",
            "Epoch 20/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9984 - val_loss: 0.0146 - val_accuracy: 0.9982\n",
            "Epoch 21/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.9983 - val_loss: 0.0145 - val_accuracy: 0.9982\n",
            "Epoch 22/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9981 - val_loss: 0.0145 - val_accuracy: 0.9982\n",
            "Epoch 23/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9984 - val_loss: 0.0144 - val_accuracy: 0.9982\n",
            "Epoch 24/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 0.9984 - val_loss: 0.0141 - val_accuracy: 0.9982\n",
            "Epoch 25/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9982 - val_loss: 0.0142 - val_accuracy: 0.9982\n",
            "Epoch 26/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9982 - val_loss: 0.0139 - val_accuracy: 0.9982\n",
            "Epoch 27/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9984 - val_loss: 0.0139 - val_accuracy: 0.9982\n",
            "Epoch 28/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 0.9983 - val_loss: 0.0138 - val_accuracy: 0.9982\n",
            "Epoch 29/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 0.9983 - val_loss: 0.0137 - val_accuracy: 0.9982\n",
            "Epoch 30/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9984 - val_loss: 0.0137 - val_accuracy: 0.9982\n",
            "Epoch 31/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 0.9983 - val_loss: 0.0138 - val_accuracy: 0.9982\n",
            "Epoch 32/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9982 - val_loss: 0.0135 - val_accuracy: 0.9982\n",
            "Epoch 33/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9984 - val_loss: 0.0135 - val_accuracy: 0.9982\n",
            "Epoch 34/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.0135 - val_accuracy: 0.9982\n",
            "Epoch 35/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0134 - val_accuracy: 0.9982\n",
            "Epoch 36/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9985 - val_loss: 0.0135 - val_accuracy: 0.9982\n",
            "Epoch 37/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.0134 - val_accuracy: 0.9982\n",
            "Epoch 38/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9986 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 39/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 40/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9986 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 41/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0134 - val_accuracy: 0.9982\n",
            "Epoch 42/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9984 - val_loss: 0.0135 - val_accuracy: 0.9982\n",
            "Epoch 43/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 44/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 0.9982 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 45/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 46/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 47/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9984 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 48/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 49/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 50/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 51/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9984 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 52/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9983 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 53/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 54/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 55/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 56/200\n",
            "140/140 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 57/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 58/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 59/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 60/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 61/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 62/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 63/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 64/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.9981 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 65/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 66/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.9981 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
            "Epoch 67/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 68/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 69/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 70/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 71/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 72/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 73/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 74/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 75/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 76/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 77/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 78/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 79/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 80/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 81/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 82/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 83/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 84/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 85/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 86/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 87/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 88/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 89/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 90/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 91/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 92/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 93/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 94/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 95/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 96/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 97/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 98/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.0132 - val_accuracy: 0.9982\n",
            "Epoch 99/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 100/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 101/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 102/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 103/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 104/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 105/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 106/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 107/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 108/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.9981 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 109/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 110/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 111/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 112/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 113/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 114/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 115/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 116/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 117/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 118/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 119/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 120/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 121/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 122/200\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 123/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 124/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 125/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 126/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 127/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 128/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 129/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 130/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 131/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 132/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 133/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 134/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 135/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 136/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 137/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 138/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 139/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 140/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 141/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 142/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 143/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 144/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 145/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 146/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 147/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 148/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 149/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 150/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 151/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 152/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.9981 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 153/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 154/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 155/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 156/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 157/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 158/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 159/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 160/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 161/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 162/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 163/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 164/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 165/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 166/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 167/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 168/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 169/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 170/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 171/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 172/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 173/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 174/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 175/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 176/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 177/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 178/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 179/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 180/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 181/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 182/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 183/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 184/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 185/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 186/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 187/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 188/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 189/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 190/200\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 191/200\n",
            "140/140 [==============================] - 1s 4ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 192/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 193/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 194/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 195/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 196/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 197/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 198/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 199/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Epoch 200/200\n",
            "140/140 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK75Q1B1jLFb"
      },
      "source": [
        "Training_Loss = model_history.history['loss']\n",
        "Validated_Loss = model_history.history['val_loss']\n",
        "Each_Epochs=range(1,201)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tGPYizaYi6YK",
        "outputId": "5eed9152-23dc-4d91-db05-5c09f1c79ca5"
      },
      "source": [
        "plt.plot(Each_Epochs, Training_Loss, 'r', label='Training Loss')\n",
        "plt.plot(Each_Epochs, Validated_Loss, 'g', label='Validation Loss')\n",
        "plt.title('Training vs Validation Losses ')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#Looks very good as i suggested Optimizer to take lower Learning rate."
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXo73FTbjFxq"
      },
      "source": [
        "**8.Prediction** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNBvakoLk30x",
        "outputId": "b6f929f2-5967-451a-9a46-544a3ec1ab7a"
      },
      "source": [
        "network.predict(x_test_30)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00153476],\n",
              "       [0.00153476],\n",
              "       [0.00153476],\n",
              "       ...,\n",
              "       [0.00153476],\n",
              "       [0.00153476],\n",
              "       [0.00153476]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg8NQOyDk34y",
        "outputId": "9b66f21c-bb20-4a47-c413-d7f68921b64d"
      },
      "source": [
        "network.evaluate(x_test_30,y_test_30)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1336/1336 [==============================] - 1s 1ms/step - loss: 0.0143 - accuracy: 0.9980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.014273595996201038, 0.9980337619781494]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1AFTdSTlPnE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}